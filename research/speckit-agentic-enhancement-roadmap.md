# Speckit Agentic Enhancement Roadmap
## Implementing Enterprise Best Practices (2024-2026 Research)

**Project:** Doxee Speckit Requirements Automation Framework
**Date:** February 6, 2026
**Purpose:** Concrete implementation plan for enterprise agentic patterns

---

## Current State Assessment

### What Speckit Already Has (Strengths)

#### 1. Linear Phase Architecture ‚úÖ
```
specify ‚Üí clarify ‚Üí plan ‚Üí tasks ‚Üí analyze ‚Üí implement
```
**Alignment:** Matches Phase 1 + Phase 2 of best-practice three-phase model
**Gap:** Missing Phase 0 (Strategic Discovery)

#### 2. Structured Templates ‚úÖ
- `spec-template.md` with mandatory sections
- `plan-template.md` with constitution gates
- `tasks-template.md` with dependency tracking
- `checklist-template.md` for quality validation

**Alignment:** Enforces structured outputs (anti-pattern avoidance)
**Gap:** Templates need enhancement for "agent-ready" specifications

#### 3. Constitution-Based Governance ‚úÖ
- `.specify/memory/constitution.md` defines project principles
- Constitution gates in planning phase

**Alignment:** Matches "constitution pattern" from research
**Gap:** Constitution needs competitive intelligence and persona sections

#### 4. Git-Based Traceability ‚úÖ
- All artifacts in `specs/###-feature-name/`
- Branch naming convention (###-feature-name)
- Version control for all artifacts

**Alignment:** Matches enterprise traceability requirements
**Gap:** Missing metadata files for audit trails

#### 5. Task Dependency Management ‚úÖ
- Tasks.md with parallelization markers `[P]`
- User story mapping `[US#]`
- Sequential task IDs `T###`

**Alignment:** Supports multi-agent coordination
**Gap:** No confidence scoring or escalation triggers

### What's Missing (Gaps)

#### 1. Strategic Discovery Phase (Phase 0) ‚ùå
**Current:** Workflow starts at `specify` with a feature description
**Best Practice:** Should start with competitive validation and synthetic user testing
**Impact:** Features lack market validation before engineering investment

#### 2. Bridge Agents ‚ùå
**Current:** All work is human-executed
**Best Practice:** Four specialized agents remove bottlenecks
**Impact:** PM, UX, QA remain bottlenecks

#### 3. Automated Quality Gates ‚ùå
**Current:** Humans manually validate specs
**Best Practice:** Spec-critic agent validates before progression
**Impact:** Low-quality specs reach implementation phase

#### 4. Confidence Scoring & Escalation ‚ùå
**Current:** No mechanism for agents to signal uncertainty
**Best Practice:** Confidence thresholds trigger human review
**Impact:** No automated risk detection

#### 5. Cost Management ‚ùå
**Current:** No token budgets or cost tracking
**Best Practice:** Per-task token budgets with escalation
**Impact:** Uncontrolled costs in production

#### 6. Executable Evals ‚ùå
**Current:** Checklists are human-readable only
**Best Practice:** Machine-executable test scenarios
**Impact:** Cannot automate quality validation

#### 7. Living Knowledge Base Management ‚ùå
**Current:** No formal process for maintaining context
**Best Practice:** Context Curator role with audit tooling
**Impact:** Context rot over time

---

## Enhancement Roadmap: Three Phases

---

## PHASE 1: Foundation (Weeks 1-4)
**Goal:** Implement critical missing pieces without disrupting current workflow

### Week 1: Template Enhancement

#### Enhancement 1.1: Agent-Ready Spec Template
**File:** `.specify/templates/spec-template.md`

**Add New Sections:**

```markdown
## Visual Blueprint (MANDATORY)
**Status:** [ ] AI-Generated [ ] Human-Created [ ] Approved by UX

**Mockup Link/Attachment:** [URL or file path]

**Component Inventory:**
- [ ] Component ID: `DoxeeButton` (from design system)
- [ ] Component ID: `DoxeeDataGrid` (from design system)
- [ ] Component ID: `DoxeeModal` (from design system)

**Interaction Flow:**
1. User clicks "Filter" button
2. Modal opens with filter form
3. User submits ‚Üí Toast notification ‚Üí Grid refreshes

---

## Technical Toolset (MANDATORY)
**API Endpoints:**
- `GET /api/v1/invoices` - Returns invoice list
  - Auth: Bearer token required
  - Params: `{ startDate, endDate, status }`
- `POST /api/v1/invoices` - Creates new invoice
  - Schema: See `contracts/invoice-schema.json`

**State Management:**
- Pattern: React Query for server state
- Local state: useState for form inputs

**Dependencies:**
- Must NOT block: T001, T002
- Blocks: T010 (payment processing requires invoices)

---

## Executable Acceptance Scenarios (MANDATORY)
**Scenario 1: Happy Path**
```gherkin
Given user is authenticated as "Admin"
And database has 10 invoices in date range
When user selects date range "2026-01-01" to "2026-01-31"
And user clicks "Apply Filter"
Then API call to GET /api/v1/invoices is triggered
And response status is 200
And DataGrid displays exactly 10 rows
And loading spinner is NOT visible
And toast shows "Filters applied successfully"
```

**Scenario 2: Error Handling**
```gherkin
Given user is authenticated
And API returns 500 error
When user clicks "Apply Filter"
Then DataGrid shows empty state
And error message displays "Unable to load invoices. Please try again."
And error is logged to monitoring system
```

---

## Context References (MANDATORY)
**Relevant Documentation:**
- Architecture: `docs/architecture/api-design.md`
- Design System: `docs/design-system/components.md`
- Similar Features: `specs/045-customer-filtering/spec.md`

**Constraints:**
- GDPR: Do not log invoice amounts
- Performance: Must load <2 seconds for 1000 rows
- Browser Support: Chrome, Firefox, Safari (latest 2 versions)

---

## Agent Readiness Checklist
- [ ] Visual blueprint attached and validated
- [ ] All API endpoints documented with examples
- [ ] All UI components from design system (no custom components)
- [ ] At least 3 executable scenarios (happy path + 2 error cases)
- [ ] Context documentation linked
- [ ] No ambiguous language ("fast" ‚Üí specify milliseconds)
```

#### Enhancement 1.2: Metadata Template
**New File:** `.specify/templates/metadata-template.json`

```json
{
  "spec_id": "[SPEC_ID]",
  "feature_name": "[FEATURE_NAME]",
  "business_case_link": "",
  "created_at": "[TIMESTAMP]",
  "created_by": "[USER_EMAIL]",
  "agent_metadata": {
    "ai_model": "claude-opus-4-6",
    "constitution_version": "",
    "templates_version": ""
  },
  "approvals": {
    "spec": {
      "approved_by": "",
      "approved_at": "",
      "confidence_score": 0.0
    },
    "plan": {
      "approved_by": "",
      "approved_at": "",
      "confidence_score": 0.0
    },
    "tasks": {
      "approved_by": "",
      "approved_at": "",
      "confidence_score": 0.0
    }
  },
  "metrics": {
    "total_tokens_used": 0,
    "total_cost_usd": 0.0,
    "generation_time_seconds": 0,
    "iterations_required": 0
  },
  "compliance": {
    "gdpr_reviewed": false,
    "security_reviewed": false,
    "accessibility_reviewed": false
  }
}
```

**Implementation:** Auto-generate on `speckit.specify`, update at each phase.

### Week 2: Constitution Enhancement

#### Enhancement 2.1: Competitive Intelligence Section
**File:** `.specify/memory/constitution.md`

**Add Section:**

```markdown
## Competitive Intelligence Matrix

<competitive_intelligence>
  <competitor name="Quadient" threat_level="high">
    <products>Inspire Platform (CCM), Inspire Flex</products>
    <strengths>
      - Large legacy install base (Fortune 500)
      - Hybrid deployment (cloud + on-premise)
      - Strong document composition engine
    </strengths>
    <weaknesses>
      - Complex user interface (steep learning curve)
      - Expensive upgrade path from on-premise to cloud
      - Limited interactive experience capabilities
      - Slow time-to-value (6-12 month implementations)
    </weaknesses>
    <response_strategy>
      - Attack on UX simplicity: "Doxee delivers value in weeks, not months"
      - Emphasize cloud-native agility: "No migration pain, born in the cloud"
      - Differentiate on interactive: "We transform documents into experiences"
    </response_strategy>
  </competitor>

  <competitor name="OpenText" threat_level="medium">
    <products>OpenText Exstream, Core Content</products>
    <strengths>
      - Enterprise-grade feature completeness
      - Strong compliance and governance tools
      - Wide third-party integrations
    </strengths>
    <weaknesses>
      - Monolithic architecture (hard to customize)
      - High total cost of ownership
      - Limited innovation velocity
    </weaknesses>
    <response_strategy>
      - Attack on TCO: "Lower cost, faster ROI"
      - Emphasize API-first: "Integrate in days, not months"
      - Differentiate on innovation: "Modern tech stack, modern features"
    </response_strategy>
  </competitor>

  <competitor name="Smart Communications" threat_level="medium">
    <products>SmartCOMM (CCM)</products>
    <strengths>
      - Cloud-native architecture
      - User-friendly interface
      - Strong Salesforce integration
    </strengths>
    <weaknesses>
      - Limited personalization capabilities
      - No video personalization offering
      - Smaller market presence
    </weaknesses>
    <response_strategy>
      - Differentiate on Pvideo: "We offer what they can't: video at scale"
      - Match on cloud-native: "Equally modern, more innovative"
      - Leverage Doxee ix: "Interactive experiences, not just documents"
    </response_strategy>
  </competitor>
</competitive_intelligence>

## Feature Priority Matrix

<feature_matrix>
  <feature name="Batch Document Generation" importance="table-stakes">
    <competitive_status>Parity required</competitive_status>
    <rule>Must match Quadient throughput: 100k docs/hour minimum</rule>
    <priority>P2 (Must-have but not differentiator)</priority>
  </feature>

  <feature name="Interactive Personalized Video (Pvideo)" importance="differentiator">
    <competitive_status>Market leader</competitive_status>
    <rule>Must exceed all competitors. Invest heavily.</rule>
    <priority>P1 (Strategic differentiation)</priority>
  </feature>

  <feature name="Omni-channel Distribution" importance="table-stakes">
    <competitive_status>Parity required</competitive_status>
    <rule>Email, SMS, Print, Portal - all competitors have this</rule>
    <priority>P2 (Must-have)</priority>
  </feature>

  <feature name="Personalized Web Microsites (Pweb)" importance="differentiator">
    <competitive_status>Advantage</competitive_status>
    <rule>Strong differentiator, continue innovation</rule>
    <priority>P1 (Strategic differentiation)</priority>
  </feature>

  <feature name="E-invoicing Compliance (Doxee px)" importance="specialized">
    <competitive_status>Niche leader (Italy)</competitive_status>
    <rule>Must maintain compliance leadership in Italy market</rule>
    <priority>P1 (Market-specific critical)</priority>
  </feature>

  <feature name="API-First Integration" importance="table-stakes">
    <competitive_status>Advantage vs legacy, parity vs cloud-native</competitive_status>
    <rule>All features must expose REST APIs with OpenAPI specs</rule>
    <priority>P1 (Strategic enabler)</priority>
  </feature>
</feature_matrix>
```

#### Enhancement 2.2: Persona Context Section
**Add to Constitution:**

```markdown
## User Personas & Jobs-to-be-Done

<personas>
  <persona id="USR-01" role="Enterprise Admin" vertical="Utilities" archetype="configurator">
    <demographics>
      - 35-50 years old
      - Technical background (IT/Operations)
      - Manages 50k-5M documents/month
    </demographics>
    <jtbd>
      When I need to launch a new billing campaign,
      I want to configure templates and batch jobs quickly,
      So that I can meet monthly billing deadlines without IT support.
    </jtbd>
    <pain_points>
      - Complex UI with too many clicks
      - Slow template editor (laggy with large templates)
      - Unclear error messages when jobs fail
      - No self-service troubleshooting
    </pain_points>
    <success_criteria>
      - Can configure job in <30 minutes
      - Error messages explain root cause
      - Can preview before running batch
    </success_criteria>
  </persona>

  <persona id="USR-02" role="Integration Developer" vertical="All" archetype="integrator">
    <demographics>
      - 25-40 years old
      - Full-stack developer
      - Integrates Doxee into CRM/ERP/Billing systems
    </demographics>
    <jtbd>
      When I need to integrate Doxee into our systems,
      I want clear API documentation with code examples,
      So that I can complete integration in days, not weeks.
    </jtbd>
    <pain_points>
      - Incomplete API documentation
      - No sandbox environment for testing
      - Breaking changes without versioning
      - Poor error messages from APIs
    </pain_points>
    <success_criteria>
      - Complete integration in <5 days
      - Never blocked waiting for support
      - APIs are self-documenting
    </success_criteria>
  </persona>

  <persona id="USR-03" role="End Customer" vertical="Utilities" archetype="consumer">
    <demographics>
      - 18-80 years old
      - Low to high technical literacy
      - Receives bills/statements via Doxee
    </demographics>
    <jtbd>
      When I receive my bill,
      I want to understand my charges and take action easily,
      So that I can manage my account without calling support.
    </jtbd>
    <pain_points>
      - Bills are confusing (complex tables)
      - Hard to find "pay now" button
      - Not mobile-friendly
      - No way to ask questions inline
    </pain_points>
    <success_criteria>
      - Can understand bill in <2 minutes
      - Can pay from mobile device
      - No need to call support
    </success_criteria>
  </persona>

  <persona id="USR-04" role="Marketing Manager" vertical="Telco" archetype="strategist">
    <demographics>
      - 30-50 years old
      - Business/Marketing background
      - Manages customer retention campaigns
    </demographics>
    <jtbd>
      When I need to reduce churn,
      I want to send personalized video messages to at-risk customers,
      So that I can improve retention without increasing support costs.
    </jtbd>
    <pain_points>
      - Cannot personalize at scale
      - No way to A/B test messaging
      - Cannot track video engagement
      - Slow turnaround (weeks to launch campaign)
    </pain_points>
    <success_criteria>
      - Launch campaign in <48 hours
      - Personalize 100k+ videos per batch
      - Track engagement metrics in real-time
    </success_criteria>
  </persona>
</personas>
```

### Week 3: Script Enhancements

#### Enhancement 3.1: Confidence Scoring Script
**New File:** `.specify/scripts/bash/calculate-confidence.sh`

```bash
#!/bin/bash
# Calculate confidence score for a spec artifact

set -euo pipefail

source "$(dirname "$0")/common.sh"

SPEC_DIR="${1:-}"
ARTIFACT="${2:-spec.md}"  # spec.md, plan.md, or tasks.md

if [[ -z "$SPEC_DIR" ]]; then
    echo "Usage: $0 <spec-directory> [artifact]"
    exit 1
fi

ARTIFACT_PATH="$SPEC_DIR/$ARTIFACT"

if [[ ! -f "$ARTIFACT_PATH" ]]; then
    echo "Error: Artifact not found: $ARTIFACT_PATH"
    exit 1
fi

# Confidence criteria checklist
score=0
max_score=10

# Check 1: Has visual blueprint (mockup/screenshot)
if grep -q "Visual Blueprint" "$ARTIFACT_PATH" && grep -q -E "\[.*\.(png|jpg|jpeg|svg|pdf|figma)\]" "$ARTIFACT_PATH"; then
    ((score+=2))
fi

# Check 2: Has API documentation with examples
if grep -q "API Endpoints" "$ARTIFACT_PATH" && grep -q -E "GET|POST|PUT|DELETE" "$ARTIFACT_PATH"; then
    ((score+=2))
fi

# Check 3: Has executable scenarios (Gherkin Given/When/Then)
if grep -q "Given" "$ARTIFACT_PATH" && grep -q "When" "$ARTIFACT_PATH" && grep -q "Then" "$ARTIFACT_PATH"; then
    ((score+=2))
fi

# Check 4: Has context documentation links
if grep -q "Context References" "$ARTIFACT_PATH" && grep -q -E "docs/|specs/" "$ARTIFACT_PATH"; then
    ((score+=1))
fi

# Check 5: No ambiguous language
ambiguous_count=$(grep -iE "fast|slow|easy|simple|intuitive|user-friendly|should|might|probably" "$ARTIFACT_PATH" | wc -l | xargs)
if [[ "$ambiguous_count" -lt 3 ]]; then
    ((score+=1))
fi

# Check 6: All mandatory sections present
mandatory_sections=("User Stories" "Acceptance Scenarios" "Technical" "Context")
for section in "${mandatory_sections[@]}"; do
    if grep -q "$section" "$ARTIFACT_PATH"; then
        ((score+=1))
    fi
done

# Calculate confidence percentage
confidence=$(echo "scale=2; $score / $max_score" | bc)

# Output
echo "Confidence Score: $confidence"
echo "Points: $score / $max_score"

# Threshold check
threshold=0.85
if (( $(echo "$confidence < $threshold" | bc -l) )); then
    echo "‚ö†Ô∏è  BELOW THRESHOLD: Requires human review"
    exit 1
else
    echo "‚úÖ PASSED: Confidence threshold met"
    exit 0
fi
```

#### Enhancement 3.2: Cost Tracking Script
**New File:** `.specify/scripts/bash/track-costs.sh`

```bash
#!/bin/bash
# Track token usage and costs for a feature

set -euo pipefail

source "$(dirname "$0")/common.sh"

SPEC_DIR="${1:-}"

if [[ -z "$SPEC_DIR" ]]; then
    echo "Usage: $0 <spec-directory>"
    exit 1
fi

METADATA_FILE="$SPEC_DIR/.metadata.json"

if [[ ! -f "$METADATA_FILE" ]]; then
    echo "Error: Metadata file not found: $METADATA_FILE"
    exit 1
fi

# Extract cost data
total_tokens=$(jq -r '.metrics.total_tokens_used' "$METADATA_FILE")
total_cost=$(jq -r '.metrics.total_cost_usd' "$METADATA_FILE")

# Budget configuration (from config file or defaults)
CONFIG_FILE=".specify/config/budgets.json"
if [[ -f "$CONFIG_FILE" ]]; then
    budget=$(jq -r '.feature_budget_usd' "$CONFIG_FILE")
    warn_threshold=$(jq -r '.cost_alerts.warn_at_percent / 100' "$CONFIG_FILE")
else
    budget=10.00  # Default $10 per feature
    warn_threshold=0.80
fi

# Calculate percentage
percent=$(echo "scale=2; $total_cost / $budget" | bc)

# Output
echo "Feature: $(basename "$SPEC_DIR")"
echo "Tokens Used: $total_tokens"
echo "Cost: \$$total_cost / \$$budget ($percent%)"

# Alert logic
if (( $(echo "$percent >= 1.0" | bc -l) )); then
    echo "üö® BUDGET EXCEEDED: Feature cost exceeds budget"
    exit 1
elif (( $(echo "$percent >= $warn_threshold" | bc -l) )); then
    echo "‚ö†Ô∏è  WARNING: Approaching budget limit"
    exit 0
else
    echo "‚úÖ Within budget"
    exit 0
fi
```

### Week 4: Command Enhancements

#### Enhancement 4.1: New Command - `/speckit.discover`
**New File:** `.claude/commands/speckit.discover.md`

```markdown
# /speckit.discover

## Purpose
Phase 0: Strategic Discovery. Validate business ideas before committing to specification.

## Usage
```
/speckit.discover <business-idea-description>
```

## What It Does

1. **Competitive Positioning Check**
   - Analyzes against competitors in constitution
   - Identifies overlaps and differentiation opportunities
   - Flags "table-stakes" vs "differentiator" features

2. **Persona Validation**
   - Maps idea to user personas
   - Identifies which JTBD the idea addresses
   - Highlights potential pain points solved

3. **Feasibility Estimate**
   - Scans codebase for similar implementations
   - Identifies required APIs and components
   - Provides rough complexity score (Low/Medium/High)

4. **Opportunity Solution Tree (OST) Generation**
   - Desired outcome (from business idea)
   - Opportunities (problems to solve)
   - Solutions (potential approaches)
   - Experiments (validation steps)

## Output

Creates: `specs/discovery/[date]-[idea-slug]/`
- `business-case.md` - Executive summary
- `competitive-analysis.md` - Market positioning
- `persona-mapping.md` - User impact analysis
- `feasibility-report.md` - Technical assessment
- `opportunity-solution-tree.md` - Visual OST

## Next Steps

After review, human PM decides:
- ‚úÖ Proceed ‚Üí Run `/speckit.specify` with validated idea
- ‚ùå Kill ‚Üí Archive in `specs/discovery/archived/`
- üîÑ Refine ‚Üí Update business case and re-run

## Example

```
/speckit.discover "Add real-time chat support widget to personalized bills so customers can ask questions without calling support"
```

Output:
```
‚úÖ Competitive Analysis:
  - Quadient: No inline chat (gap opportunity)
  - SmartComm: Has chat but not contextual (differentiation opportunity)
  - Assessment: Differentiator feature

‚úÖ Persona Mapping:
  - Primary: USR-03 (End Customer) - Addresses pain point "no way to ask questions inline"
  - Secondary: USR-01 (Enterprise Admin) - Reduces support call volume

‚úÖ Feasibility:
  - Complexity: Medium
  - Existing components: ChatWidget (needs integration)
  - Required APIs: /api/v1/chat/init, /api/v1/chat/message
  - Similar features: specs/078-support-ticketing/

‚úÖ Recommendation: PROCEED with specification
  - High user value (reduces support costs)
  - Competitive differentiator
  - Medium implementation effort
  - Aligns with Doxee ix "interactive experience" strategy
```
```

#### Enhancement 4.2: Enhanced Command - `/speckit.plan`
**Modify:** `.claude/commands/speckit.plan.md`

**Add Confidence Check Before Planning:**

```markdown
## Prerequisites (Enhanced)

Before running `plan`:

1. ‚úÖ Spec exists and is approved
2. ‚úÖ **Confidence score >= 0.85** (NEW)
3. ‚úÖ Visual blueprint attached
4. ‚úÖ At least 3 executable scenarios
5. ‚úÖ All API endpoints documented

**Automatic Check:**
```bash
$ /speckit.plan
Running confidence check...
‚ö†Ô∏è Confidence: 0.72 (Below threshold)

Issues detected:
- Missing visual blueprint (add mockup or screenshot)
- Only 1 acceptance scenario (minimum 3 required)
- API endpoint /api/v2/payments not documented

Please resolve issues before proceeding to plan phase.
```

**Override (for human judgment):**
```bash
$ /speckit.plan --force
‚ö†Ô∏è Forcing plan generation despite low confidence score
```
```

---

## PHASE 2: Bridge Agents (Weeks 5-8)
**Goal:** Implement four specialized agents to remove bottlenecks

### Week 5: UI-Drafter Agent

#### Agent 2.1: UI Mockup Generator
**New Command:** `/speckit.mockup`

**Implementation:**
```markdown
# /speckit.mockup

## Purpose
Generate design-system-compliant UI mockups from text descriptions.

## Usage
```
/speckit.mockup <ui-description>
```

## What It Does

1. **Parse Description**
   - Extract layout requirements
   - Identify interaction patterns
   - Determine component needs

2. **Component Selection**
   - Search Doxee Design System for matching components
   - Flag if custom components needed (requires UX approval)
   - Validate component compatibility

3. **Mockup Generation**
   - Use v0.dev or Bolt.new for visual generation
   - Apply Doxee design tokens (colors, spacing, typography)
   - Generate responsive layouts (mobile/tablet/desktop)

4. **Output Artifacts**
   - SVG mockup file
   - Component inventory (list of Doxee components used)
   - Interaction flow diagram

## Example

```
/speckit.mockup "Invoice list page with filters sidebar on left, data grid in center with sortable columns, and export button in top-right toolbar"
```

Output:
- `mockups/invoice-list-page.svg` (visual mockup)
- `mockups/invoice-list-components.json` (component inventory)
- Auto-attached to spec.md Visual Blueprint section

## Workflow Integration

1. PM writes feature description
2. PM runs `/speckit.mockup <description>`
3. Agent generates mockup
4. PM reviews (10 min vs 2 days waiting for UX)
5. UX Designer reviews and stamps approval (30 min vs creating from scratch)
6. Approved mockup goes into spec
```

**Technical Implementation:**
- Integrate with v0.dev API or Bolt.new
- Load Doxee design system tokens from `.specify/design-system/tokens.json`
- Use Claude for component selection logic

### Week 6: Spec-Critic Agent

#### Agent 2.2: Automated Quality Gate
**Implementation:** Enhance `/speckit.plan` with pre-flight check

**New Script:** `.specify/scripts/bash/validate-spec-quality.sh`

```bash
#!/bin/bash
# Validate spec quality before allowing progression to plan phase

set -euo pipefail

source "$(dirname "$0")/common.sh"

SPEC_FILE="${1:-}"

if [[ -z "$SPEC_FILE" ]]; then
    echo "Usage: $0 <spec-file>"
    exit 1
fi

echo "Running Spec Quality Validation..."

# Check 1: Confidence score
if ! .specify/scripts/bash/calculate-confidence.sh "$(dirname "$SPEC_FILE")" "$(basename "$SPEC_FILE")"; then
    echo "‚ùå FAILED: Low confidence score"
    exit 1
fi

# Check 2: No placeholder text
if grep -q "\[NEEDS CLARIFICATION\]" "$SPEC_FILE"; then
    echo "‚ùå FAILED: Unresolved clarifications remain"
    exit 1
fi

if grep -q "\[TODO\]" "$SPEC_FILE"; then
    echo "‚ùå FAILED: TODO items remain"
    exit 1
fi

# Check 3: Visual blueprint validation
if ! grep -q "Visual Blueprint" "$SPEC_FILE"; then
    echo "‚ùå FAILED: Missing Visual Blueprint section"
    exit 1
fi

if ! grep -q -E "\[.*\.(png|jpg|jpeg|svg|pdf|figma)\]" "$SPEC_FILE"; then
    echo "‚ùå FAILED: No mockup attached"
    exit 1
fi

# Check 4: API validation (check if endpoints exist in codebase)
api_endpoints=$(grep -oE "GET|POST|PUT|DELETE /api/[^ ]*" "$SPEC_FILE" || true)
if [[ -n "$api_endpoints" ]]; then
    echo "Validating API endpoints against codebase..."
    # Search for endpoint definitions in code
    while read -r endpoint; do
        if ! grep -rq "$endpoint" src/ backend/ api/ 2>/dev/null; then
            echo "‚ö†Ô∏è  WARNING: API endpoint not found in codebase: $endpoint"
        fi
    done <<< "$api_endpoints"
fi

# Check 5: Component validation (check if components exist in design system)
components=$(grep -oE "Component ID: \`[^` ]*\`" "$SPEC_FILE" | sed 's/Component ID: `\(.*\)`/\1/' || true)
if [[ -n "$components" ]]; then
    echo "Validating components against design system..."
    if [[ -f ".specify/design-system/components.json" ]]; then
        while read -r component; do
            if ! jq -e ".components[] | select(.id == \"$component\")" .specify/design-system/components.json >/dev/null 2>&1; then
                echo "‚ö†Ô∏è  WARNING: Component not found in design system: $component"
            fi
        done <<< "$components"
    fi
fi

# Check 6: Persona validation
if grep -q "As a" "$SPEC_FILE"; then
    personas=$(grep -oE "As a [^,]*" "$SPEC_FILE" | sed 's/As a //' || true)
    while read -r persona; do
        if ! grep -q "$persona" .specify/memory/constitution.md; then
            echo "‚ö†Ô∏è  WARNING: Persona not defined in constitution: $persona"
        fi
    done <<< "$personas"
fi

echo "‚úÖ PASSED: Spec quality validation successful"
exit 0
```

**Integration:** Automatically run before `/speckit.plan` command.

### Week 7: Doc-Hunter Agent

#### Agent 2.3: Context Retrieval Automation
**New Command:** `/speckit.findcontext`

```markdown
# /speckit.findcontext

## Purpose
Automatically retrieve relevant documentation for a feature specification.

## Usage
```
/speckit.findcontext <spec-directory>
```

## What It Does

1. **Keyword Extraction**
   - Parse spec.md for domain terms
   - Extract API endpoints, component names, feature areas
   - Identify similar past features

2. **Documentation Search**
   - Search `docs/` directory (architecture, API docs, guides)
   - Search `specs/` for similar implemented features
   - Search `doxee/` for product documentation

3. **Relevance Ranking**
   - Score documents by keyword match
   - Prioritize recent documents
   - Boost documents referenced by similar specs

4. **Output**
   - Auto-populate "Context References" section in spec.md
   - Generate `research.md` with deeper context
   - Create `similar-features.md` with implementation patterns

## Example

```
$ /speckit.findcontext specs/089-invoice-filtering/

Searching for relevant context...

Found 12 relevant documents:

High Relevance:
- docs/architecture/api-design.md (score: 0.92)
- specs/045-customer-filtering/spec.md (score: 0.88)
- doxee/official-product-documentation/data-transformation (score: 0.85)

Medium Relevance:
- docs/design-system/datagrid-component.md (score: 0.72)
- specs/078-export-functionality/plan.md (score: 0.68)

Updated spec.md with context references.
Generated specs/089-invoice-filtering/research.md
```

## Workflow Integration

1. PM writes draft spec
2. PM runs `/speckit.findcontext`
3. Agent populates context automatically
4. Context Curator (Scrum Master) reviews and validates (5 min vs 2 hours of manual searching)
5. Proceed to `/speckit.plan`
```

**Technical Implementation:**
- Use vector search (embeddings) for semantic matching
- Hybrid search: combine vector similarity + keyword matching
- Store document embeddings in `.specify/knowledge/embeddings.db`
- Update embeddings weekly (cron job or `/speckit.context rebuild`)

### Week 8: Eval-Gen Agent

#### Agent 2.4: Executable Test Generator
**New Command:** `/speckit.genevals`

```markdown
# /speckit.genevals

## Purpose
Generate executable test code from acceptance scenarios in spec.

## Usage
```
/speckit.genevals <spec-directory>
```

## What It Does

1. **Parse Acceptance Scenarios**
   - Extract Given/When/Then scenarios from spec.md
   - Identify testable assertions
   - Determine test type (unit, integration, E2E)

2. **Generate Test Code**
   - Pytest for backend APIs
   - Playwright for frontend E2E
   - Jest for component unit tests

3. **Generate Test Data**
   - Create fixture files
   - Generate mock data
   - Define test database states

4. **Output**
   - `evals/test_<feature>.py` (pytest)
   - `evals/e2e_<feature>.spec.ts` (playwright)
   - `evals/fixtures/` (test data)
   - `evals/README.md` (how to run)

## Example

Input (from spec.md):
```gherkin
Scenario 1: Happy Path
Given user is authenticated as "Admin"
And database has 10 invoices in date range
When user selects date range "2026-01-01" to "2026-01-31"
And user clicks "Apply Filter"
Then API call to GET /api/v1/invoices is triggered
And response status is 200
And DataGrid displays exactly 10 rows
```

Output (`evals/test_invoice_filtering.py`):
```python
import pytest
from playwright.sync_api import Page, expect

@pytest.fixture
def setup_test_data(db_session):
    """Create 10 test invoices in date range"""
    for i in range(10):
        db_session.add(Invoice(
            id=f"INV-{i:03d}",
            date=date(2026, 1, i+1),
            amount=100.00
        ))
    db_session.commit()

def test_invoice_filtering_happy_path(page: Page, setup_test_data):
    """
    Scenario 1: Happy Path
    Given user is authenticated as "Admin"
    """
    page.goto("http://localhost:3000/login")
    page.fill("#username", "admin")
    page.fill("#password", "password")
    page.click("#login-button")

    # Navigate to invoices page
    page.goto("http://localhost:3000/invoices")

    # When user selects date range
    page.fill("#start-date", "2026-01-01")
    page.fill("#end-date", "2026-01-31")
    page.click("#apply-filter")

    # Then DataGrid displays exactly 10 rows
    expect(page.locator(".data-grid-row")).to_have_count(10)

    # And loading spinner is not visible
    expect(page.locator(".loading-spinner")).not_to_be_visible()
```

## Workflow Integration

1. PM writes spec with acceptance scenarios
2. QA reviews scenarios for completeness
3. PM runs `/speckit.genevals`
4. Agent generates test code
5. QA reviews and adds edge cases (30 min vs 4 hours writing from scratch)
6. Tests added to CI/CD pipeline
7. Tests run on every commit
```

**Technical Implementation:**
- Parse Gherkin syntax from spec.md
- Use templates for common test patterns
- Map Gherkin keywords to code actions:
  - "Given" ‚Üí setup/fixtures
  - "When" ‚Üí user actions
  - "Then" ‚Üí assertions
- Generate both happy path and error scenario tests

---

## PHASE 3: Advanced Features (Weeks 9-12)
**Goal:** Add observability, cost management, and continuous improvement

### Week 9: AgentOps Observability

#### Feature 3.1: Trace Logging
**New File:** `.specify/scripts/bash/log-agent-trace.sh`

```bash
#!/bin/bash
# Log agent execution traces for debugging

set -euo pipefail

COMMAND="${1:-}"
SPEC_DIR="${2:-}"
TRACE_FILE=".specify/logs/traces/$(date +%Y%m%d-%H%M%S)-$COMMAND.json"

mkdir -p .specify/logs/traces

# Capture execution metadata
cat > "$TRACE_FILE" <<EOF
{
  "timestamp": "$(date -Iseconds)",
  "command": "$COMMAND",
  "spec_directory": "$SPEC_DIR",
  "user": "$USER",
  "git_branch": "$(git branch --show-current 2>/dev/null || echo 'unknown')",
  "git_commit": "$(git rev-parse HEAD 2>/dev/null || echo 'unknown')",
  "steps": []
}
EOF

echo "Trace log started: $TRACE_FILE"
echo "$TRACE_FILE"
```

**Integration:** Wrap all slash commands to automatically log traces.

#### Feature 3.2: Failure Analysis Dashboard
**New Command:** `/speckit.analyze-failures`

```markdown
# /speckit.analyze-failures

## Purpose
Analyze agent failures to identify patterns and improvement opportunities.

## Usage
```
/speckit.analyze-failures [--last-n-days 30]
```

## What It Does

1. **Collect Failure Data**
   - Parse trace logs for errors
   - Identify failed confidence checks
   - Catalog human rejections

2. **Pattern Detection**
   - Most common failure reasons
   - Which commands fail most often
   - Which specs require most iterations

3. **Root Cause Analysis**
   - Missing documentation (doc-hunter failures)
   - API endpoint mismatches
   - Design system gaps

4. **Recommendations**
   - Documentation to add
   - Constitution updates needed
   - Template improvements

## Output

```
Failure Analysis Report (Last 30 Days)
========================================

Total Agent Runs: 247
Failed Runs: 23 (9.3%)

Top Failure Reasons:
1. Low confidence score (35%) - Spec missing visual blueprint
2. API endpoint not found (26%) - /api/v2/payments not in codebase
3. Component not in design system (17%) - CustomDropdown used instead of DoxeeSelect
4. Ambiguous language (13%) - Terms like "fast", "intuitive" detected
5. Missing persona (9%) - Persona "Data Analyst" not in constitution

Recommendations:
‚úÖ Add visual blueprint requirement to spec template (prevents 35% of failures)
‚úÖ Update API documentation for /api/v2/payments
‚úÖ Add CustomDropdown to design system or deprecate usage
‚úÖ Create style guide for quantifiable language
‚úÖ Add "Data Analyst" persona to constitution

Cost Impact:
- Failed runs wasted $127.35 in tokens
- Average cost per failure: $5.54
- Projected savings: $1,524.20/year if top 3 issues resolved
```
```

### Week 10: Cost Management System

#### Feature 3.3: Budget Configuration
**New File:** `.specify/config/budgets.json`

```json
{
  "version": "1.0",
  "feature_budget_usd": 10.00,
  "command_budgets": {
    "discover": {
      "max_tokens": 100000,
      "max_cost_usd": 3.00,
      "description": "Strategic discovery with competitive analysis"
    },
    "specify": {
      "max_tokens": 50000,
      "max_cost_usd": 1.50,
      "description": "Initial specification generation"
    },
    "clarify": {
      "max_tokens": 20000,
      "max_cost_usd": 0.60,
      "description": "Clarification iterations (max 3)"
    },
    "plan": {
      "max_tokens": 80000,
      "max_cost_usd": 2.40,
      "description": "Technical planning and architecture"
    },
    "tasks": {
      "max_tokens": 30000,
      "max_cost_usd": 0.90,
      "description": "Task decomposition"
    },
    "implement": {
      "max_tokens": 100000,
      "max_cost_usd": 3.00,
      "description": "Code generation per phase"
    }
  },
  "cost_alerts": {
    "warn_at_percent": 80,
    "block_at_percent": 100,
    "notify_email": "product-ops@doxee.com"
  },
  "pricing": {
    "model": "claude-opus-4-6",
    "input_token_cost": 0.000015,
    "output_token_cost": 0.000075,
    "updated_at": "2026-02-06"
  }
}
```

#### Feature 3.4: Real-Time Cost Tracking
**Integration:** Wrap all agent calls with cost tracking

```bash
# Before agent call
start_tokens=$(get_current_token_count)

# Agent execution
run_agent_command

# After agent call
end_tokens=$(get_current_token_count)
tokens_used=$((end_tokens - start_tokens))
cost=$(calculate_cost $tokens_used)

# Update metadata
update_feature_metadata "$SPEC_DIR" "$tokens_used" "$cost"

# Check budget
if ! check_budget "$SPEC_DIR"; then
    echo "‚ö†Ô∏è  Budget warning: Approaching feature limit"
    send_alert "product-ops@doxee.com" "Feature $SPEC_DIR approaching budget"
fi
```

### Week 11: Context Audit System

#### Feature 3.5: Automated Context Freshness Check
**New Command:** `/speckit.context audit`

```markdown
# /speckit.context audit

## Purpose
Validate that documentation in the knowledge base is accurate and up-to-date.

## Usage
```
/speckit.context audit [--fix]
```

## What It Does

1. **Scan Documentation**
   - Find all files in `docs/`, `doxee/`, and `specs/`
   - Check last modified date
   - Identify documents not updated in >6 months

2. **Validate Links**
   - Find all internal links
   - Check if targets exist
   - Flag broken links

3. **Check API Documentation**
   - Extract API endpoints from docs
   - Search codebase for implementations
   - Flag mismatches

4. **Validate Examples**
   - Find code examples in docs
   - Check if syntax is still valid
   - Flag deprecated patterns

5. **Generate Report**
   - Context freshness score (0-100)
   - List of stale documents
   - List of broken links
   - List of deprecated examples

## Output

```
Context Audit Report
====================

Overall Score: 73/100 (Needs Improvement)

Stale Documents (>6 months):
‚ö†Ô∏è  docs/architecture/auth-flow.md (last updated: 2025-06-01)
‚ö†Ô∏è  docs/api/invoice-api.md (last updated: 2025-04-15)
‚ö†Ô∏è  doxee/Doxee-platform-old.md (last updated: 2024-11-20)

Broken Links:
‚ùå specs/045-customer-filtering/spec.md references docs/components/filter.md (not found)
‚ùå docs/api/payment-api.md references specs/deprecated/payment-v1/ (not found)

API Mismatches:
‚ùå docs/api/invoice-api.md documents GET /api/v1/invoices but codebase has /api/v2/invoices

Deprecated Examples:
‚ö†Ô∏è  docs/examples/react-class-components.md uses class components (hooks are now standard)

Recommendations:
1. Update or archive 3 stale documents
2. Fix 2 broken links
3. Update API documentation for v2 endpoints
4. Modernize React examples

Run with --fix to automatically:
- Archive documents not modified in >12 months
- Update API documentation from OpenAPI specs
- Fix broken links to moved files
```

## --fix Mode

```
/speckit.context audit --fix

Fixing issues...
‚úÖ Archived docs/deprecated/old-auth-flow.md ‚Üí docs/archive/
‚úÖ Updated docs/api/invoice-api.md with v2 endpoints
‚úÖ Fixed link in specs/045-customer-filtering/spec.md
‚úÖ Added deprecation warning to docs/examples/react-class-components.md

3 issues auto-fixed, 1 issue requires manual review.
```
```

### Week 12: Continuous Improvement System

#### Feature 3.6: Correction Loop Implementation
**New Command:** `/speckit.correct`

```markdown
# /speckit.correct

## Purpose
Capture human corrections to agent outputs and use them to improve future runs.

## Usage
```
/speckit.correct <artifact-file> <issue-description>
```

## What It Does

1. **Log Correction**
   - Record what was wrong
   - Record human's fix
   - Tag with issue category

2. **Store as Few-Shot Example**
   - Add to `.specify/memory/corrections/`
   - Index for retrieval
   - Use in future similar scenarios

3. **Update Constitution (if applicable)**
   - If correction reveals missing principle
   - Propose constitution amendment

4. **Generate Improvement Report**
   - Track correction patterns
   - Identify recurring issues
   - Suggest systematic fixes

## Example

```
$ /speckit.correct specs/089-invoice-filtering/spec.md "Agent used ambiguous language 'fast loading' instead of '<2 seconds'"

Correction logged.

Category: Ambiguous language
Previous pattern: "fast loading"
Corrected pattern: "<2 seconds (quantified)"

Stored as few-shot example: .specify/memory/corrections/002-quantify-performance.md

Similar past corrections:
- specs/067-dashboard/spec.md: "quick response" ‚Üí "<500ms"
- specs/078-export/spec.md: "large files" ‚Üí ">10MB"

Pattern detected: Agent often uses subjective performance terms.

Recommendation: Add rule to constitution:
"Performance requirements must be quantified with specific metrics (milliseconds, seconds, MB, etc.)"

Apply to constitution? [y/N]
```

**Correction Storage:**
```markdown
# Correction 002: Quantify Performance Requirements

## Context
Feature: Invoice Filtering (specs/089-invoice-filtering/)
Date: 2026-02-06
Command: /speckit.specify

## Issue
Agent wrote: "The grid should load fast to provide good user experience"

## Problem
"Fast" is subjective and not testable.

## Correction
Should be: "The grid must load in <2 seconds for datasets up to 1000 rows"

## Rule
All performance requirements must include:
1. Specific metric (seconds, milliseconds, MB, etc.)
2. Test conditions (dataset size, network speed, etc.)
3. Measurable threshold

## Examples of Good Performance Requirements
- "Page load time < 2 seconds on 3G network"
- "API response time < 500ms for 95th percentile"
- "File export completes in <10 seconds for 10,000 rows"

## Tags
- ambiguous-language
- performance
- testability
```
```

**Integration:** Before each agent run, retrieve top 5 relevant corrections and include in context.

---

## PHASE 4: Rollout and Training (Weeks 13-16)

### Week 13: Pilot Team Setup
- Select one feature for full agentic workflow
- Train PM, QA, Dev on new commands
- Run end-to-end: discover ‚Üí specify ‚Üí plan ‚Üí tasks ‚Üí implement
- Measure: cycle time, rework rate, satisfaction

### Week 14: Documentation and Runbooks
- Create "Agentic PM Handbook" for Doxee
- Document all new commands and workflows
- Create troubleshooting guide
- Record training videos

### Week 15: Team Rollout
- Extend to 3 teams
- Monitor metrics dashboard
- Collect feedback
- Iterate on agent prompts and thresholds

### Week 16: Optimization
- Analyze failure patterns
- Tune confidence thresholds
- Update constitution based on learnings
- Celebrate wins and lessons learned

---

## Success Metrics (KPIs)

### Phase 1 (Weeks 1-4)
- [ ] All specs follow enhanced template
- [ ] Metadata files auto-generated for all new features
- [ ] Constitution includes competitive intelligence and personas
- [ ] Confidence scoring implemented and tested

### Phase 2 (Weeks 5-8)
- [ ] UI-Drafter agent generates mockups in <2 minutes
- [ ] Spec-Critic agent blocks low-quality specs (0% escape rate)
- [ ] Doc-Hunter agent retrieves relevant docs with >85% accuracy
- [ ] Eval-Gen agent generates executable tests for >80% of scenarios

### Phase 3 (Weeks 9-12)
- [ ] All agent runs traced and logged
- [ ] Failure analysis dashboard operational
- [ ] Token budgets enforced for all commands
- [ ] Context audit runs monthly automatically
- [ ] Correction loop captures and reuses human feedback

### Phase 4 (Weeks 13-16)
- [ ] Pilot feature shipped end-to-end with agentic workflow
- [ ] Cycle time reduced by >50% vs baseline
- [ ] Rework rate <10%
- [ ] Team satisfaction score >4/5
- [ ] Cost per feature within budget (95% of features)

---

## Risk Mitigation

### Risk 1: Agent Quality Degradation
**Mitigation:**
- Monthly context audits (automated)
- Context Curator role with explicit responsibilities
- Golden Dataset validation before production

### Risk 2: Cost Overruns
**Mitigation:**
- Token budgets with hard limits
- 80% warning threshold
- Cost tracking dashboard
- Monthly cost reviews

### Risk 3: Team Resistance
**Mitigation:**
- Start with enthusiastic early adopters
- Show quick wins (mockup generation)
- Emphasize "augmentation not replacement"
- Provide excellent training and support

### Risk 4: Compliance Concerns
**Mitigation:**
- Implement audit trails from day 1
- Human-signed requirement rule
- Golden Dataset validation
- Regular compliance reviews

### Risk 5: Over-Reliance on AI
**Mitigation:**
- HITL gates at critical decisions
- Confidence thresholds for escalation
- Human always makes final call
- Regular human-only reviews to maintain skills

---

## Next Steps

### Immediate (Week 1)
1. Review and approve this roadmap with leadership
2. Assign Context Curator role
3. Budget for AgentOps tools (LangSmith/Arize)
4. Select pilot team and feature

### Short-Term (Weeks 2-4)
1. Implement Phase 1 enhancements
2. Train pilot team
3. Run first agent-ready spec
4. Collect feedback and iterate

### Medium-Term (Weeks 5-12)
1. Build and deploy bridge agents
2. Implement observability and cost management
3. Scale to 3 teams
4. Measure and optimize

### Long-Term (Months 4-6)
1. Full team rollout
2. Fine-tune based on correction loop data
3. Explore proprietary model fine-tuning
4. Publish case study (Doxee as agentic PM leader)

---

**Prepared by:** AI Research Team
**For:** Doxee Speckit Product Team
**Date:** February 6, 2026
**Status:** Ready for leadership review and approval
**Est. Implementation Time:** 16 weeks (4 months)
**Est. ROI:** 2-3x feature throughput, 50-70% cycle time reduction
